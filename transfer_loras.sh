#!/bin/bash

# Create remote directories
ssh -p 14347 -i ~/.ssh/my_custom_key root@103.196.86.29 "mkdir -p ~/loras/ilya_repin_style ~/loras/src/lora_ops/src ~/loras/repin_lora"

# First, create a tar archive excluding large safetensors files
cd ~/loras
tar --exclude="*.safetensors" --exclude="sdxl-base-1.0" --exclude="venv" -czf /tmp/loras_transfer.tar.gz .

# Transfer the archive
scp -P 14347 -i ~/.ssh/my_custom_key /tmp/loras_transfer.tar.gz root@103.196.86.29:/tmp/

# Extract on the remote server
ssh -p 14347 -i ~/.ssh/my_custom_key root@103.196.86.29 "cd ~/loras && tar -xzf /tmp/loras_transfer.tar.gz"

# Check if images are present, if not add a note
IMAGE_COUNT=$(ssh -p 14347 -i ~/.ssh/my_custom_key root@103.196.86.29 "ls -1 ~/loras/ilya_repin_style/*.jpg 2>/dev/null | wc -l")
if [ "$IMAGE_COUNT" -eq 0 ]; then
    echo "⚠️ No images found in the remote ilya_repin_style directory! You need to transfer your Ilya Repin images."
    echo "Use this command to transfer images:"
    echo "scp -P 14347 -i ~/.ssh/my_custom_key ~/loras/ilya_repin_style/*.jpg root@103.196.86.29:~/loras/ilya_repin_style/"
fi

# Clean up
rm /tmp/loras_transfer.tar.gz
ssh -p 14347 -i ~/.ssh/my_custom_key root@103.196.86.29 "rm /tmp/loras_transfer.tar.gz"

echo "==============================================================="
echo "Transfer complete! All files except safetensors and SDXL base model transferred."
echo ""
echo "To run on server:"
echo "1. ssh -p 14347 -i ~/.ssh/my_custom_key root@103.196.86.29"
echo "2. cd ~/loras"
echo ""
echo "3. Install dependencies:"
echo "   apt-get update && apt-get install -y python3-pip python3-venv python3-dev build-essential"
echo "   python3 -m venv venv"
echo "   source venv/bin/activate"
echo "   pip install --upgrade pip"
echo "   pip install numpy==1.24.3"
echo "   # Install PyTorch 2.2.1 which is compatible with stable xformers"
echo "   pip install torch==2.2.1 torchvision==0.17.1 --index-url https://download.pytorch.org/whl/cu121"
echo "   pip install diffusers transformers safetensors pillow tqdm accelerate huggingface_hub"
echo "   # Install compatible xformers for memory efficient attention"
echo "   pip install xformers==0.0.23"
echo ""
echo "   # Install Rust and maturin for the Rust LoRA module:"
echo "   curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y"
echo "   source \$HOME/.cargo/env"
echo "   pip install maturin"
echo "   cd ~/loras/src/lora_ops && maturin develop && cd ~/loras"
echo ""
echo "4. Download minimal SDXL (required for training):"
echo "   python download_minimal_sdxl.py"
echo "   python download_tokenizers.py"
echo ""
echo "5. Process Ilya Repin style images and start training:"
echo "   python process_repin.py"
echo "   python fix_vae.py"
echo ""
echo "6. Run optimized training for Ilya Repin style:"
echo "   python train_full_lora.py \\"
echo "     --output_dir=repin_lora \\"
echo "     --images_dir=processed_repin \\"
echo "     --captions_file=repin_captions.json \\"
echo "     --lora_name=repin_style \\"
echo "     --num_train_epochs=50 \\"
echo "     --rank=32 \\"
echo "     --learning_rate=1e-4 \\"
echo "     --train_batch_size=8 \\"
echo "     --gradient_accumulation_steps=1 \\"
echo "     --use_text_conditioning=True \\"
echo "     --cache_latents \\"
echo "     --mixed_precision=bf16 \\"
echo "     --enable_xformers_memory_efficient_attention \\"
echo "     --use_rust=True \\"
echo "     --save_every_n_steps=25"
echo ""
echo "7. To check training progress from another terminal:"
echo "   ssh -p 14347 -i ~/.ssh/my_custom_key root@103.196.86.29 \"tail -f ~/loras/repin_lora/train.log\""
echo ""
echo "8. To download your trained LoRA file back to your local machine:"
echo "   scp -P 14347 -i ~/.ssh/my_custom_key root@103.196.86.29:~/loras/repin_lora/repin_style.safetensors ~/Downloads/"
echo ""
echo "9. To test your LoRA with comparisons:"
echo "   python compare_lora.py \\"
echo "     --lora_path=repin_lora/repin_style.safetensors \\"
echo "     --prompt=\"Portrait of a person in the style of classical Russian painting, detailed\" \\"
echo "     --output_dir=repin_comparison \\"
echo "     --num_comparisons=1 \\"
echo "     --alpha=0.8"
echo ""
echo "10. To download the comparison images:"
echo "    scp -P 14347 -i ~/.ssh/my_custom_key \"root@103.196.86.29:~/loras/repin_comparison/*.png\" ~/Downloads/"
echo ""
echo "11. To run training with nohup (continues even if connection drops):"
echo "    cd ~/loras && source venv/bin/activate && nohup python train_full_lora.py \\"
echo "      --output_dir=repin_lora \\"
echo "      --images_dir=processed_repin \\"
echo "      --captions_file=repin_captions.json \\"
echo "      --lora_name=repin_style \\"
echo "      --num_train_epochs=50 \\"
echo "      --rank=32 \\"
echo "      --learning_rate=1e-4 \\"
echo "      --train_batch_size=8 \\"
echo "      --gradient_accumulation_steps=1 \\"
echo "      --use_text_conditioning=True \\"
echo "      --cache_latents \\"
echo "      --mixed_precision=bf16 \\"
echo "      --use_rust=True \\"
echo "      --save_every_n_steps=25 \\"
echo "      > repin_lora/training.log 2>&1 &"
echo ""
echo "    # Check progress with:"
echo "    tail -f repin_lora/training.log"
echo ""
echo "    # If you need to stop the training:"
echo "    ps aux | grep train_full_lora.py"
echo "    kill [PROCESS_ID]"
echo ""
echo "===============================================================" 